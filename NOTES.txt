~~~~~~~
NOTES:
~~~~~~~
-> closed-world classifier f divides into two sub-networks:
   - Encoder E and classifier C
      ~ E: X->Z, where X is image space, Z is latent space.
         i) Z is a vector space R^d
      ~ C: Z->\delta_K, where \delta_K is the probability simplex
        over the K known classes.

-> Open category detector g:X->[0,1] will be divided into the same
Encoder E and scoring function S, where S:Z->[0,1].
    - So, the classifier f and the open category detector g share
    the same latent space Z (Presumably will be generated by
    Autoencoders)
    - h is multiclass version of this

-> Learning a good anomaly detector g is thus decomposed into the
problem of learning a good latent representation Z=E(X) and a good
scoring function S.

~~~~~~~~~~~~~~~~~~~~~~~
PROCESS AT HIGH LEVEL:
~~~~~~~~~~~~~~~~~~~~~~~
Given test query x, eval g(x) and compare it to an anomaly score
threshold \tao (0.5). If g(x) >= \tao, x is declared to belong
to an unknown class, and an appropriate action, such as raising an
alarm, is taken.

Otherwise, closed-world classifier f is applied to predict a
probability distribution over the known classes.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ORACLE REPRESENTATION LEARNING:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-> To obtain oracle latent representation, train network f on
all classes (known and unknown).

~~~~~~~~~~~~~~~~~~~~~~
IMPLEMENTATION NOTES:
~~~~~~~~~~~~~~~~~~~~~~
-> The trained autoencoder uses reconstruction error as its
loss function. This is described in Section 3.2.4 of Risheek's
thesis.

-> Current differences between my implementation and
Risheek's:
    - Using K-Fold Cross Validation instead of
    validation set



-> Consider using seed for reproducibility
   '''
   import torch
   torch.manual_seed(0)
   '''

-> For f classifier (Resnet?) check out Alex and Matt's Repo
or take a closer look at:
https://pytorch-tutorial.readthedocs.io/en/latest/
tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/

-> Linear scoring function based on the latent representation of
a neural network.

-+-+-+-+-+-+-+-+-+-+-+-+-+-+10/22/2020+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

~~~~~~~~~~
Progress:
~~~~~~~~~~
-> Preprocessing
   - Modified Counterfactual open set data processing scripts
   to provide nice train, val, test split
   
   - Outputs 10 different .csv representing 5 different
   known/unknown splits

-> Latent Representations
   - Trained two convolutional autoencoders
       - 1 on known
       - 1 on known/unknown (gratuitous?)

-> Working on implementing OMD so that we can train linear
anomaly detector using latent space representation of "known"
autoencoder.
   - If I understand correctly, feedback is provided by looking
   directly at validation label and determining if training
   example corresponds to known or unknown class?

-> Using pyod LODA to get prior (prototyped, but not tested; need
latent dataset first)

-> Working on building latent dataset from "known" autoencoder.
Doing research on Conv2D semantics to figure this out.
""torch.Size([1, 64, 2, 2])""
    - should I squeeze this into a list?
    - Maybe smaller latent representation... 32x32 -> 32x8 here

~~~~~~~~~~~
Questions:
~~~~~~~~~~~
-> Normalization?
   - For latent rep. models?
   - For linear anomaly detector?

-> Is the feedback just coming from a known/unknown membership check
by peeking at the validation .csv?

-> How would you recommend implementing optimization step?
   - Brute force?
   - Specialized solver?

-> What is S in OMD algorithm? Is this just any convex set?

-> Would you suggest building dataframe that holds all latent
representations with 'known'/'unknown' classification in
first column?
    - Does starting with binary case seem reasonable?
    - Does OMD anomaly discovery

-+-+-+-+-+-+-+-+-+-+-+-+-+-+10/29/2020+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

~~~~~~~~~~~
Progress:
~~~~~~~~~~~
->

~~~~~~~~~~~
Questions:
~~~~~~~~~~~
->
