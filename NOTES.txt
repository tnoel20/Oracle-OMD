~~~~~~~
NOTES:
~~~~~~~
-> closed-world classifier f divides into two sub-networks:
   - Encoder E and classifier C
      ~ E: X->Z, where X is image space, Z is latent space.
         i) Z is a vector space R^d
      ~ C: Z->\delta_K, where \delta_K is the probability simplex
        over the K known classes.

-> Open category detector g:X->[0,1] will be divided into the same
Encoder E and scoring function S, where S:Z->[0,1].
    - So, the classifier f and the open category detector g share
    the same latent space Z (Presumably will be generated by
    Autoencoders)
    - h is multiclass version of this

-> Learning a good anomaly detector g is thus decomposed into the
problem of learning a good latent representation Z=E(X) and a good
scoring function S.

~~~~~~~~~~~~~~~~~~~~~~~
PROCESS AT HIGH LEVEL:
~~~~~~~~~~~~~~~~~~~~~~~
Given test query x, eval g(x) and compare it to an anomaly score
threshold \tao (0.5). If g(x) >= \tao, x is declared to belong
to an unknown class, and an appropriate action, such as raising an
alarm, is taken.

Otherwise, closed-world classifier f is applied to predict a
probability distribution over the known classes.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
ORACLE REPRESENTATION LEARNING:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
-> To obtain oracle latent representation, train network f on
all classes (known and unknown).

~~~~~~~~~~~~~~~~~~~~~~
IMPLEMENTATION NOTES:
~~~~~~~~~~~~~~~~~~~~~~
-> The trained autoencoder uses reconstruction error as its
loss function. This is described in Section 3.2.4 of Risheek's
thesis.

-> Current differences between my implementation and
Risheek's:
    - Using K-Fold Cross Validation instead of
    validation set



-> Consider using seed for reproducibility
   '''
   import torch
   torch.manual_seed(0)
   '''

-> For f classifier (Resnet?) check out Alex and Matt's Repo
or take a closer look at:
https://pytorch-tutorial.readthedocs.io/en/latest/
tutorial/chapter03_intermediate/3_2_2_cnn_resnet_cifar10/

-> Linear scoring function based on the latent representation of
a neural network.

-+-+-+-+-+-+-+-+-+-+-+-+-+-+10/22/2020+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

~~~~~~~~~~
Progress:
~~~~~~~~~~
-> Preprocessing
   - Modified Counterfactual open set data processing scripts
   to provide nice train, val, test split
   
   - Outputs 10 different .csv representing 5 different
   known/unknown splits

-> Latent Representations
   - Trained two convolutional autoencoders
       - 1 on known
       - 1 on known/unknown (gratuitous?)

-> Working on implementing OMD so that we can train linear
anomaly detector using latent space representation of "known"
autoencoder.
   - If I understand correctly, feedback is provided by looking
   directly at validation label and determining if training
   example corresponds to known or unknown class?

-> Using pyod LODA to get prior (prototyped, but not tested; need
latent dataset first)

-> Working on building latent dataset from "known" autoencoder.
Doing research on Conv2D semantics to figure this out.
""torch.Size([1, 64, 2, 2])""
    - should I squeeze this into a list?
    - Maybe smaller latent representation... 32x32 -> 32x8 here

~~~~~~~~~~~
Questions:
~~~~~~~~~~~
-> Normalization?
   - For latent rep. models?
   - For linear anomaly detector?

-> Is the feedback just coming from a known/unknown membership check
by peeking at the validation .csv?

-> How would you recommend implementing optimization step?
   - Brute force?
   - Specialized solver?

-> What is S in OMD algorithm? Is this just any convex set?

-> Would you suggest building dataframe that holds all latent
representations with 'known'/'unknown' classification in
first column?
    - Does starting with binary case seem reasonable?
    - Does OMD anomaly discovery

-+-+-+-+-+-+-+-+-+-+-+-+-+-+10/29/2020+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

~~~~~~~~~~~
Progress:
~~~~~~~~~~~
-> Built latent data representation using known+uknown validation sets
in preparation for training linear anomaly detector
    - Currently 256 features per image, but could reduce this
    if necessary!

-> Trial 1 took all night, very large weights... Not promising
    - Fixed

-> Got LODA weight prior by averaging across histograms

~~~~~~~~~~~
Questions:
~~~~~~~~~~~
-> RE: LODA weight prior
    Should weights be inverted here? (i.e. since dealing
    with histograms, high weights correspond to larger
    probability of occurrence. Isn't this the opposite of
    what we are trying to capture?)
