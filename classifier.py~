import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import scipy
import math
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from torchvision import datasets, transforms
from oc_data_load import CIFAR10_Data

# Classifier to be trained on CIFAR-10.

# Implement RESNET-18
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.embedding_net = nn.Sequential(
            self.conv1 = nn.Conv2d(3, 6, 5)
            self.pool = nn.MaxPool2d(2, 2)
            self.conv2 = nn.Conv2d(6, 16, 5)
            self.fc1 = nn.Linear(16 * 5 * 5, 120)
            self.fc2 = nn.Linear(120, 84)
        )
        
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 16 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def get_latent(self, x):
        return self.embedding_net(x)


def train(model, device, tr_data, val, num_epochs=10, learning_rate=1e-3):#batch_size=64 
    torch.manual_seed(42)
    criterion = nn.MSELoss() # mean square error loss
    optimizer = torch.optim.Adam(model.parameters(),
                                 lr=learning_rate, 
                                 weight_decay=1e-5) # <--
    outputs = []
    epoch = 1
    # Initial conditions
    val_loss_list = [1E6, 0]
    EPS = 1e-3
    #for epoch in range(num_epochs):
    while abs(val_loss_list[epoch] - val_loss_list[epoch-1]) > EPS and epoch < num_epochs:
        for i,img_batch in enumerate(tr_data):
            img_batch = img_batch.to(device)
            recon = model(img_batch)
            loss = criterion(recon, img_batch)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        print('Epoch:{}, Loss:{:.4f}'.format(epoch, float(loss)))
        outputs.append((epoch, img_batch, recon),)
        _, val_loss = get_val_loss(model, val, device)
        val_loss_list.append(val_loss)
        epoch += 1
    return outputs

# TODO: Fix this...
def get_val_loss(model, val, device):
    criterion = nn.MSELoss()
    outputs = []
    loss = 0

    # TODO: Update for val. data_loader no longer works here!!!
    for img_batch in val:
        # Reshape mini-batch data to [N, 32*32] matrix
        # Load it to the active device
        # batch_features = batch_features.view(-1, 32*32).to(device)
        img_batch = img_batch.to(device)
        # compute reconstructions
        reconstruction = model(img_batch)

        # compute training reconstruction loss
        test_loss = criterion(reconstruction, img_batch)

        # add the mini-batch training loss to epoch loss
        loss += test_loss.item()

    # Compute the epoch training loss
    loss = loss / len(val)
    # display the epoch test loss
    print("dev loss = {:.6f}".format(loss))
    outputs.append((None,img_batch,reconstruction),)

    return outputs, loss
    

def training_progression(outputs):
    num_epochs = len(outputs)
    for k in range(0, num_epochs, 5):
        plt.figure(figsize=(9,2))
        imgs = outputs[k][1].detach().numpy()
        recon = outputs[k][2].detach().numpy() #.cpu().numpy()
        for i, item in enumerate(imgs):
            if i >= 9: break
            plt.subplot(2,9,i+1)
            plt.imshow(item[0])
        for i, item in enumerate(recon):
            if i >= 9: break
            plt.subplot(2,9,9+i+1)
            plt.imshow(item[0])
    plt.show()


def get_vanilla_ae(tr=None, val=None, filename='plain_ae.pth'):
    CIFAR10_DIM = 32*32
    NUM_EPOCHS = 7

    tr_loader = torch.utils.data.DataLoader(tr, batch_size=4, shuffle=True, pin_memory=True)
    val_loader = torch.utils.data.DataLoader(val, batch_size=4, shuffle=True, pin_memory=True)
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    model = Autoencoder().to(device)
    
    if os.path.isfile(filename):
        # Load model
        model.load_state_dict(torch.load(filename))
        model.eval()
    else:
        # Train model
        outputs = train(model, device, tr_loader, val_loader, num_epochs=NUM_EPOCHS)
        torch.save(model.state_dict(), filename)

    return model


def imshow(img):
    img = img / 2 + 0.5 # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1,2,0)))
    plt.show()

    
def main():
    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,
                                              shuffle=True, num_workers=2)

    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=4,
                                             shuffle=False, num_workers=2)

    classes = ('plane', 'car', 'bird', 'cat',
               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    # get some random training images
    dataiter = iter(trainloader)
    images, labels = dataiter.next()

    # show images
    imshow(torchvision.utils.make_grid(images))
    # print labels
    print(' '.join('%5s' % classes[labels[j]] for j in range(4)))

    
    
if __name__ == '__main__':
    main()
